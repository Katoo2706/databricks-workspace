{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba4257e-15a0-43d9-af9b-d9cbb77a9077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ðŸ“š What is Delta Live Tables (DLT)?\n",
    "\n",
    "**Delta Live Tables lets you:** \\\n",
    "âœ… Define ETL pipelines declaratively (SQL or Python) \\\n",
    "âœ… Automate data quality, retries, schema inference (Delta Lake + DLT Expectations API) \\\n",
    "âœ… Track data lineage, monitoring, and versioning built-in \\\n",
    "âœ… Support both batch and streaming data \\\n",
    "âœ… Handles orchestration, state management, and checkpoints automatically\n",
    "\n",
    "> _In short: it simplifies and productionizes pipelines on Databricks with less code + more guarantees_\n",
    "> _(No need to write explicit checkpoints, upserts, etc. manually like Structured Streaming)_\n",
    "\n",
    ">> Delta Live Tables workflow only avaiable in Premium Tier. Where we can configure the Pipeline mode (Triggered or Continious)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5403d7-54d6-4cd9-b307-018e82ebfa0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "| **Aspect** | **LIVE Table (Materialized View)** | **Streaming LIVE Table** |\n",
    "| --- | --- | --- |\n",
    "| **Definition** | Table fully computed to optimize resources. | Table processes only new data, avoids recomputing old data. |\n",
    "| **Data Processing** | Batch processing, may use incremental for optimization. | Incremental processing, real-time or near real-time. |\n",
    "| **Update Mechanism** | Updates via scheduled or manual pipeline runs. | Continuous updates for new data via pipeline runs. |\n",
    "| **Use Case** | Batch processing, data warehousing, historical analysis. | Real-time analytics, IoT, log processing. |\n",
    "| **Performance** | Resource-intensive if fully recomputed. | More efficient, processes only new data. |\n",
    "| **Stateful** | Not stateful, recomputes based on current data. | Stateful, maintains state across updates. |\n",
    "| **Flow Type** | Batch flows with batch semantics. | Streaming flows with append or apply changes. |\n",
    "| **Definition Method** | Implicitly defined via batch query. | Explicitly or implicitly defined in streaming pipeline. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2a24bb-5334-480b-8762-4f5c2b307986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f467e1e-12c8-492b-93e0-b26454f583e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "orders_raw"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- cloud_files method enable Auto Loader natively in SQL.\n",
    "\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE orders_raw\n",
    "COMMENT \"The raw books orders, ingested from orders-raw\"\n",
    "AS SELECT * FROM cloud_files(\"${datasets.path}/orders-json-raw\", \"json\",\n",
    "                             map(\"cloudFiles.inferColumnTypes\", \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e243a41-b87a-4f43-b959-ed5fc85d5dba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "customers"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REFRESH LIVE TABLE customers\n",
    "COMMENT \"The customers lookup table, ingested from customers-json\"\n",
    "AS SELECT * FROM json.`${datasets.path}/customers-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a2eaa6-cbf6-4c68-98d0-f72bdac15655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fd5995-68a4-4d3d-b54d-3a15105496aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Manage data quality with Delta Lake and DLT Expectations API:**\n",
    "\n",
    "> Constraint violation\n",
    "\n",
    "| **`ON VIOLATION`** | Behavior |\n",
    "| --- | --- |\n",
    "| **`DROP ROW`** | Discard records that violate constraints |\n",
    "| **`FAIL UPDATE`** | Violated constraint causes the pipeline to fail  |\n",
    "| Omitted | Records violating constraints will be kept, and reported in metrics |\n",
    "\n",
    "_Note: We need to use LIVE prefix for LIVE tables, STREAM for STREAMING table_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f164c5b8-1101-4c9e-a02e-eccf7196f534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "/*\n",
    "  This Delta Live Table (DLT) script creates or refreshes a streaming table called `orders_cleaned`.\n",
    "  - It continuously processes data from the `orders_raw` streaming source.\n",
    "  - A data quality constraint is applied to ensure `order_id` is not null; rows violating this rule are dropped.\n",
    "  - The table enriches raw order data by joining it with customer data from the `customers` table.\n",
    "  - It selects and transforms key fields:\n",
    "      * Extracts customer first and last names from nested JSON fields.\n",
    "      * Converts the `order_timestamp` from Unix epoch seconds to a human-readable timestamp.\n",
    "      * Retrieves customer country information.\n",
    "  - This results in a clean, enriched dataset of book orders with valid order IDs, ready for downstream analytics.\n",
    "*/\n",
    "\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE orders_cleaned (\n",
    "  CONSTRAINT valid_order_number EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"The cleaned books orders with valid order_id\"\n",
    "AS\n",
    "  SELECT order_id, quantity, o.customer_id, c.profile:first_name as f_name, c.profile:last_name as l_name,\n",
    "         cast(from_unixtime(order_timestamp, 'yyyy-MM-dd HH:mm:ss') AS timestamp) order_timestamp, o.books,\n",
    "         c.profile:address:country as country\n",
    "  FROM STREAM(LIVE.orders_raw) o\n",
    "  LEFT JOIN LIVE.customers c\n",
    "    ON o.customer_id = c.customer_id\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7939613839078560,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta Live Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
