{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2061a6f-bf51-47c9-82f3-40f6e6665cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "def path_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def download_dataset(source, target):\n",
    "    files = dbutils.fs.ls(source)\n",
    "\n",
    "    for f in files:\n",
    "        source_path = f\"{source}/{f.name}\"\n",
    "        target_path = f\"{target}/{f.name}\"\n",
    "        if not path_exists(target_path):\n",
    "            print(f\"Copying {f.name} ...\")\n",
    "            dbutils.fs.cp(source_path, target_path, True)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "data_source_uri = \"s3://dalhussein-courses/datasets/bookstore/v1/\"\n",
    "dataset_bookstore = 'dbfs:/mnt/demo-datasets/bookstore'\n",
    "data_catalog = 'hive_metastore'\n",
    "spark.conf.set(f\"dataset.bookstore\", dataset_bookstore)\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.eu-west-3.amazonaws.com\")\n",
    "spark.conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_index(dir):\n",
    "    files = dbutils.fs.ls(dir)\n",
    "    index = 0\n",
    "    if files:\n",
    "        file = max(files).name\n",
    "        index = int(file.rsplit('.', maxsplit=1)[0])\n",
    "    return index+1\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def set_current_catalog(catalog_name):\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Structured Streaming\n",
    "streaming_dir = f\"{dataset_bookstore}/orders-streaming\"\n",
    "raw_dir = f\"{dataset_bookstore}/orders-raw\"\n",
    "\n",
    "def load_file(current_index):\n",
    "    latest_file = f\"{str(current_index).zfill(2)}.parquet\"\n",
    "    print(f\"Loading {latest_file} file to the bookstore dataset\")\n",
    "    dbutils.fs.cp(f\"{streaming_dir}/{latest_file}\", f\"{raw_dir}/{latest_file}\")\n",
    "\n",
    "    \n",
    "def load_new_data(all=False):\n",
    "    index = get_index(raw_dir)\n",
    "    if index >= 10:\n",
    "        print(\"No more data to load\\n\")\n",
    "\n",
    "    elif all == True:\n",
    "        while index <= 10:\n",
    "            load_file(index)\n",
    "            index += 1\n",
    "    else:\n",
    "        load_file(index)\n",
    "        index += 1\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DLT\n",
    "streaming_orders_dir = f\"{dataset_bookstore}/orders-json-streaming\"\n",
    "streaming_books_dir = f\"{dataset_bookstore}/books-streaming\"\n",
    "\n",
    "raw_orders_dir = f\"{dataset_bookstore}/orders-json-raw\"\n",
    "raw_books_dir = f\"{dataset_bookstore}/books-cdc\"\n",
    "\n",
    "def load_json_file(current_index):\n",
    "    latest_file = f\"{str(current_index).zfill(2)}.json\"\n",
    "    print(f\"Loading {latest_file} orders file to the bookstore dataset\")\n",
    "    dbutils.fs.cp(f\"{streaming_orders_dir}/{latest_file}\", f\"{raw_orders_dir}/{latest_file}\")\n",
    "    print(f\"Loading {latest_file} books file to the bookstore dataset\")\n",
    "    dbutils.fs.cp(f\"{streaming_books_dir}/{latest_file}\", f\"{raw_books_dir}/{latest_file}\")\n",
    "\n",
    "    \n",
    "def load_new_json_data(all=False):\n",
    "    index = get_index(raw_orders_dir)\n",
    "    if index >= 10:\n",
    "        print(\"No more data to load\\n\")\n",
    "\n",
    "    elif all == True:\n",
    "        while index <= 10:\n",
    "            load_json_file(index)\n",
    "            index += 1\n",
    "    else:\n",
    "        load_json_file(index)\n",
    "        index += 1\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "download_dataset(data_source_uri, dataset_bookstore)\n",
    "set_current_catalog(data_catalog)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Copy-Datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
