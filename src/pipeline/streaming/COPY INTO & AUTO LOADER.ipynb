{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b041671-eaa6-4038-8dd6-8d7dea0fee2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## COPY INTO\n",
    "\n",
    "\n",
    "ðŸŸ¢ Best for: Batch ingestion of semi-static files that donâ€™t change frequently\n",
    "ðŸŸ  Limitation: No built-in auto-detection of new files (you manually run COPY INTO periodically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d1c9381-6319-42ec-b8cf-d32a69f701cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Databricks SQL: COPY INTO Example\n",
    "-- ====================================\n",
    "-- COPY INTO allows you to load data from cloud storage into Delta table (batch ingestion)\n",
    "-- Supported formats: CSV, JSON, PARQUET, AVRO, ORC, TEXT\n",
    "-- COPY INTO is **idempotent**: it tracks already loaded files to avoid duplicates\n",
    "\n",
    "-- Example: Load CSV sales data from Azure Blob Storage / AWS S3 / ADLS\n",
    "-- NOTE: Replace 'your_container', 'your_storage_account', and 'your_path' appropriately\n",
    "\n",
    "COPY INTO sales\n",
    "FROM 'abfss://your_container@your_storage_account.dfs.core.windows.net/bookstore-data/sales/'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "-- Key Options Explained:\n",
    "-- ======================\n",
    "-- FILEFORMAT = CSV | PARQUET | JSON | AVRO  --> Specifies file type\n",
    "-- FORMAT_OPTIONS: header, inferSchema, delimiter, etc.  --> CSV-specific options\n",
    "-- COPY_OPTIONS('mergeSchema'='true')  --> Automatically update schema if new columns appear\n",
    "-- COPY INTO skips already loaded files by tracking file metadata internally (transaction log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b97c72-7630-4a2c-a47c-82b921eec804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Auto Loader (Spark Structured Streaming)\n",
    "\n",
    "ðŸŸ¢ Best for: Streaming ingestion of continuously arriving files\n",
    "ðŸŸ  Advantage: Auto Loader automatically detects new files â€” no need to trigger manually\n",
    "ðŸ”µ Supports schema evolution (with .option(\"mergeSchema\", \"true\") if needed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03712535-0825-4ba6-b823-80986470a36f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks PySpark: Auto Loader Example\n",
    "# =========================================\n",
    "# Auto Loader automatically **detects and ingests new files** arriving in cloud storage\n",
    "# Supports formats: CSV, JSON, PARQUET, AVRO, BINARY, ORC, TEXT\n",
    "# Incrementally loads data using file notifications (cloud native) or directory listing\n",
    "\n",
    "# Example: Load JSON sales data from S3 bucket (or ADLS / Azure Blob / GCS)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "# 1ï¸âƒ£ Define schema (recommended â€” better performance than inferring schema on each microbatch)\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"book_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# 2ï¸âƒ£ Use Auto Loader to read new JSON files incrementally\n",
    "sales_stream_df = (\n",
    "    spark.readStream.format(\"cloudFiles\")  # cloudFiles = Auto Loader\n",
    "        .option(\"cloudFiles.format\", \"json\")  # File format is JSON\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Optional if schema is not predefined\n",
    "        .schema(sales_schema)\n",
    "        .load(\"s3://your_bucket/bookstore-data/sales/\")  # Adjust path for your cloud provider\n",
    ")\n",
    "\n",
    "# 3ï¸âƒ£ Write streaming data into Delta table (append mode)\n",
    "sales_stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/sales_autoloader/\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .table(\"sales\")  # Target Delta table\n",
    "\n",
    "# Key Options Explained:\n",
    "# ======================\n",
    "# cloudFiles.format = csv | json | parquet | avro  --> Specifies file type\n",
    "# cloudFiles.inferColumnTypes = true  --> Optional auto schema inference (better to define schema explicitly)\n",
    "# checkpointLocation  --> Required to enable fault-tolerance & progress tracking\n",
    "# outputMode = append  --> Append only (typical for file ingestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52b5fdfb-58be-4ddb-b532-8697772e5364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Auto Loader single-command version to ingest JSON into Delta table\n",
    "\n",
    "(\n",
    "  spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .schema(\"\"\"\n",
    "      sale_id INT,\n",
    "      book_id INT,\n",
    "      customer_id INT,\n",
    "      quantity INT,\n",
    "      sale_date TIMESTAMP\n",
    "    \"\"\")\n",
    "    .load(\"s3://your_bucket/bookstore-data/sales/\")  # Adjust path (S3 / ADLS / Blob)\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/sales_autoloader/\")\n",
    "    .table(\"sales\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "COPY INTO & AUTO LOADER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
